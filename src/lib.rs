use std::{
    collections::VecDeque,
    sync::{Arc, Mutex, MutexGuard},
};

mod common;
mod federation;
mod reliability;
mod servicemodel;

use federation::nodeid::NodeId;
use reliability::loadbalancingcomponent::application::Application;
use reliability::loadbalancingcomponent::failoverunit::FailoverUnit;
use reliability::loadbalancingcomponent::node::Node;
use reliability::loadbalancingcomponent::service::Service;
use reliability::loadbalancingcomponent::servicetype::ServiceType;

use std::cmp::Ordering;

use anyhow::Result;
use time::PrimitiveDateTime;
use uuid::Uuid;

pub struct LoadOrMoveCost;

pub struct PLBScheduler;

pub struct EngineTimer;

pub struct Searcher;

/// Similar to the C++ implementation. This is the main entry point of the entire PLB engine.
/// It consists of all the required data structures to basically does 3 things:
///     1. Listen to update cluster info API calls
///     2. Run the PLB refresh loop
///     3. Schedule searches and solutions if required
pub struct PlacementAndLoadBalancing {
    nodes: Vec<Node>,
    apps: Vec<Application>,
    service_types: Vec<ServiceType>,
    services: Vec<Service>,
    failover_units: Vec<FailoverUnit>,
    loads: Vec<LoadOrMoveCost>,

    scheduler: PLBScheduler,
    timer: EngineTimer,

    // TODO: we might need to use a single lock for all data structures' pending updates
    node_update_queue: Arc<Mutex<VecDeque<Node>>>,
    app_update_queue: Arc<Mutex<VecDeque<Application>>>,
    service_type_update_queue: Arc<Mutex<VecDeque<ServiceType>>>,
    service_update_queue: Arc<Mutex<VecDeque<Service>>>,
    failover_unit_update_queue: Arc<Mutex<VecDeque<FailoverUnit>>>,
    load_update_queue: Arc<Mutex<VecDeque<LoadOrMoveCost>>>,
}

impl Default for PlacementAndLoadBalancing {
    fn default() -> Self {
        Self::new()
    }
}

impl PlacementAndLoadBalancing {
    /// Initializes the PlacementAndLoadBalancing object with the required cluster information:
    ///     - Nodes
    ///     - Applications
    ///     - Service types
    ///     - Services
    ///     - Failover units
    ///     - Loads or move costs
    pub fn new() -> Self {
        unimplemented!()
    }

    pub fn update_node() {
        unimplemented!()
    }

    pub fn delete_node() {
        unimplemented!()
    }

    pub fn update_application() {
        unimplemented!()
    }

    pub fn delete_application() {
        unimplemented!()
    }

    pub fn update_service_type() {
        unimplemented!()
    }

    pub fn delete_service_type() {
        unimplemented!()
    }

    pub fn update_service() {
        unimplemented!()
    }

    pub fn delete_service() {
        unimplemented!()
    }

    pub fn update_failover_unit() {
        unimplemented!()
    }

    pub fn delete_failover_unit() {
        unimplemented!()
    }

    pub fn update_load_or_move_cost() {
        unimplemented!()
    }

    /// Refresh the PLB data structures from the pending update queues.
    /// It also triggers PLBSchedular to schedule any searcher stages if any stages are due at the current timestamp of the refresh (now)
    pub fn refresh(&mut self, now: PrimitiveDateTime) -> Result<()> {
        // Update PLB internal data structures to sync with the latest cluster information
        // Keep all the locks in one scope so that they will be locked and released together
        // TODO: we might need to consider using a single mutex for all the pending queues, because there maybe internal dependencies between each structure
        // For example, failover unit may contain newly created nodes
        {
            // Process node updates
            let node_updates_clone = Arc::clone(&self.node_update_queue);
            let mut node_updates = node_updates_clone.lock().unwrap();
            self.internal_update_nodes(&mut node_updates)?;

            // Process app updates
            let app_updates_clone = Arc::clone(&self.app_update_queue);
            let mut app_updates = app_updates_clone.lock().unwrap();
            self.internal_update_apps(&mut app_updates)?;

            // Process service type updates
            let service_type_updates_clone = Arc::clone(&self.service_type_update_queue);
            let mut service_type_updates = service_type_updates_clone.lock().unwrap();
            self.internal_update_service_types(&mut service_type_updates)?;

            // TODO: implement the rest of the update sync
            todo!();
        }

        // TODO: let scheduler decide what phases will be run in this refresh
        // TODO: this should be run on each service domain, but for simplicity we can pack everything into one service domain
        todo!();

        // TODO: for each action generated by the scheduler, active PLB searcher to search for solution
        todo!();

        // TODO: give result back to FM (this will just be printing out the solution to the console for now)
        todo!();

        Ok(())
    }

    fn internal_update_nodes<'a>(
        &mut self,
        node_updates: &mut MutexGuard<'a, VecDeque<Node>>,
    ) -> Result<()> {
        unimplemented!()
    }

    fn internal_update_apps<'a>(
        &mut self,
        app_updates: &mut MutexGuard<'a, VecDeque<Application>>,
    ) -> Result<()> {
        unimplemented!()
    }

    fn internal_update_service_types<'a>(
        &mut self,
        service_type_updates: &mut MutexGuard<'a, VecDeque<ServiceType>>,
    ) -> Result<()> {
        unimplemented!()
    }

    /// Given a failover unit and 2 candicate secondary replicas, return the comparision result for promoting to primary
    /// A negative return value means Node 1 is preferred; a positive return value means Node 2 is preferred; 0 return value means
    /// 2 candidate nodes are equally preferred.
    ///
    /// The default CNFP algorithm is Dummy PLB
    /// TODO: add more CNFP algorithms and make an interface for user to choose
    pub fn compare_node_for_promotion(
        &self,
        _service_name: &str,
        _fu_id: Uuid,
        node1: NodeId,
        node2: NodeId,
    ) -> i32 {
        match node1.cmp(node2) {
            Ordering::Less => -1,
            Ordering::Equal => 0,
            Ordering::Greater => 1,
        }
    }
}
